{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.policy_net = nn.Sequential(\n",
    "                         nn.Linear(input_dim, hidden_dim*2),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(hidden_dim*2, hidden_dim),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "                         nn.Linear(input_dim, hidden_dim*2),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(hidden_dim*2, hidden_dim),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(hidden_dim, 1))\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs = torch.tensor(obs)\n",
    "        pd_params = self.policy_net(obs)\n",
    "        prob_dist = torch.distributions.Categorical(logits=pd_params)\n",
    "        action = prob_dist.sample()\n",
    "        #calculate log of probability of taking action(a_t) by the policy(pi) given the obs(s_t)\n",
    "        log_prob = prob_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def compute_state_value(self, obs):\n",
    "        obs = torch.tensor(obs)\n",
    "        state_value = self.value_net(obs)\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, agent, nb_episodes, nb_timesteps, gamma):\n",
    "    returns_no_baseline, returns_baseline = [], []\n",
    "    for episode in range(1, nb_episodes+1):\n",
    "        obs, _ = env.reset()\n",
    "        rewards, state_values = [], []\n",
    "        for timestep in range(1, nb_timesteps+1):\n",
    "            action, log_prob = agent.act(obs)\n",
    "            state_value = agent.compute_state_value(obs)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state_values.append(state_value)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        #\"Reward-to-go policy gradient\"\n",
    "        #calculate return at each time step efficiently by using dynamic programming\n",
    "        returns = []\n",
    "        future_return = 0.0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            #R[t] = r[t] + gamma * R[t+1]\n",
    "            future_return = rewards[t] + gamma * future_return\n",
    "            returns.append(future_return)\n",
    "        returns.reverse() #Now, the returns are indexed from 0 to nb_timesteps\n",
    "        returns = torch.tensor(returns)\n",
    "        state_values = torch.cat(state_values)\n",
    "\n",
    "        no_baseline = torch.sum(returns).item()\n",
    "        baseline = torch.sum(returns - state_values).item()\n",
    "        returns_no_baseline.append(no_baseline)\n",
    "        returns_baseline.append(baseline)\n",
    "    \n",
    "    return returns_no_baseline, returns_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"CartPole-v1\")\n",
    "parser.add_argument(\"--nb_episodes\", default=300)\n",
    "parser.add_argument(\"--nb_timesteps\", default=200)\n",
    "parser.add_argument(\"--gamma\", default=0.99)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "env = gym.make(args.env)\n",
    "\n",
    "agent = Agent(input_dim=env.observation_space.shape[0],\n",
    "                hidden_dim=32, out_dim=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = \"checkpoints/agent_ckpt-ep_150.pt\"\n",
    "agent.load_state_dict(torch.load(ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_no_baseline, returns_baseline = sample_trajectories(env, agent, nb_episodes=1_000_000, \n",
    "                                                            nb_timesteps=args.nb_timesteps, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_no_baseline, returns_baseline = torch.tensor(returns_no_baseline), torch.tensor(returns_baseline)\n",
    "var_no_baseline, var_baseline = torch.var(returns_no_baseline), torch.var(returns_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for no baseline: 714.624\n",
      "Variance for baseline: 341.414\n"
     ]
    }
   ],
   "source": [
    "print(f\"Variance for no baseline: {var_no_baseline:.3f}\")\n",
    "print(f\"Variance for baseline: {var_baseline:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
